{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88dd7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa43964a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_text</th>\n",
       "      <th>case_folding CPMK2</th>\n",
       "      <th>preprocessing CPMK3</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@CakKhum Ya Smga ALLÀH dg Seluruh Kekuatan ALA...</td>\n",
       "      <td>@cakkhum ya smga allàh dg seluruh kekuatan ala...</td>\n",
       "      <td>smga all h kuat alam nya jujur amp dahsyat kua...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Nexus031191 @puzzwles jangan ada wamen2 yang ...</td>\n",
       "      <td>@nexus031191 @puzzwles jangan ada wamen2 yang ...</td>\n",
       "      <td>wamen angkat pres susah ngaturnya klau dirjen ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@sharpandshark mentri purbaya uang harus dipak...</td>\n",
       "      <td>@sharpandshark mentri purbaya uang harus dipak...</td>\n",
       "      <td>tri uang pakai putar gerak ekonomi masyarakat ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sharpandshark Beda dengan mentri purbaya sang...</td>\n",
       "      <td>@sharpandshark beda dengan mentri purbaya sang...</td>\n",
       "      <td>beda tri sang tri tri uang rida ekonomi nasion...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new idol has been unlock Purbaya Yudhi Sadewaa...</td>\n",
       "      <td>new idol has been unlock purbaya yudhi sadewaa...</td>\n",
       "      <td>new idol has been unlock yudhi sadewa</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>@AnKiiim_ Waham kabeh Dokteeeeeerrr ini ada pa...</td>\n",
       "      <td>@ankiiim_ waham kabeh dokteeeeeerrr ini ada pa...</td>\n",
       "      <td>waham kabeh dokter pasien kabuurr gt</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Keren semoga bukan omon² nantinya ya pak.. Ins...</td>\n",
       "      <td>keren semoga bukan omon² nantinya ya pak.. ins...</td>\n",
       "      <td>keren moga omon insyaallah kerja hasil tunggu ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Heboh! Anak Menkeu Purbaya Sadewa Pamer Gepoka...</td>\n",
       "      <td>heboh! anak menkeu purbaya sadewa pamer gepoka...</td>\n",
       "      <td>heboh anak sadewa pamer gepok dolar as hasil t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Ngebuzzer Purbaya dulu yak :) https://t.co/Za6...</td>\n",
       "      <td>ngebuzzer purbaya dulu yak :) https://t.co/za6...</td>\n",
       "      <td>ngebuzzer yak</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>@salam4jari Mungkin saya akan menolak dengan h...</td>\n",
       "      <td>@salam4jari mungkin saya akan menolak dengan h...</td>\n",
       "      <td>tolak halus program langgar kode etik bisnis s...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             full_text  \\\n",
       "0    @CakKhum Ya Smga ALLÀH dg Seluruh Kekuatan ALA...   \n",
       "1    @Nexus031191 @puzzwles jangan ada wamen2 yang ...   \n",
       "2    @sharpandshark mentri purbaya uang harus dipak...   \n",
       "3    @sharpandshark Beda dengan mentri purbaya sang...   \n",
       "4    new idol has been unlock Purbaya Yudhi Sadewaa...   \n",
       "..                                                 ...   \n",
       "245  @AnKiiim_ Waham kabeh Dokteeeeeerrr ini ada pa...   \n",
       "246  Keren semoga bukan omon² nantinya ya pak.. Ins...   \n",
       "247  Heboh! Anak Menkeu Purbaya Sadewa Pamer Gepoka...   \n",
       "248  Ngebuzzer Purbaya dulu yak :) https://t.co/Za6...   \n",
       "249  @salam4jari Mungkin saya akan menolak dengan h...   \n",
       "\n",
       "                                    case_folding CPMK2  \\\n",
       "0    @cakkhum ya smga allàh dg seluruh kekuatan ala...   \n",
       "1    @nexus031191 @puzzwles jangan ada wamen2 yang ...   \n",
       "2    @sharpandshark mentri purbaya uang harus dipak...   \n",
       "3    @sharpandshark beda dengan mentri purbaya sang...   \n",
       "4    new idol has been unlock purbaya yudhi sadewaa...   \n",
       "..                                                 ...   \n",
       "245  @ankiiim_ waham kabeh dokteeeeeerrr ini ada pa...   \n",
       "246  keren semoga bukan omon² nantinya ya pak.. ins...   \n",
       "247  heboh! anak menkeu purbaya sadewa pamer gepoka...   \n",
       "248  ngebuzzer purbaya dulu yak :) https://t.co/za6...   \n",
       "249  @salam4jari mungkin saya akan menolak dengan h...   \n",
       "\n",
       "                                   preprocessing CPMK3     label  \n",
       "0    smga all h kuat alam nya jujur amp dahsyat kua...  positive  \n",
       "1    wamen angkat pres susah ngaturnya klau dirjen ...  positive  \n",
       "2    tri uang pakai putar gerak ekonomi masyarakat ...  positive  \n",
       "3    beda tri sang tri tri uang rida ekonomi nasion...  positive  \n",
       "4                new idol has been unlock yudhi sadewa  positive  \n",
       "..                                                 ...       ...  \n",
       "245               waham kabeh dokter pasien kabuurr gt  negative  \n",
       "246  keren moga omon insyaallah kerja hasil tunggu ...  negative  \n",
       "247  heboh anak sadewa pamer gepok dolar as hasil t...  negative  \n",
       "248                                      ngebuzzer yak  negative  \n",
       "249  tolak halus program langgar kode etik bisnis s...  negative  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33d83ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stop_words = set(stopwords.words(\"indonesian\"))\n",
    "\n",
    "stopwords_extra = {\n",
    "    \"nya\", \"sih\", \"saja\", \"aja\", \"kok\", \"sama\", \"pun\", \"lah\", \"dong\", \"deh\",\n",
    "    \"banget\", \"bgt\", \"yg\", \"ya\", \"kan\", \"kah\", \"nah\", \"loh\", \"tuh\", \"nih\",\n",
    "    \"gitu\", \"gituan\", \"begitu\", \"oke\", \"ok\", \"yaa\", \"yaaah\", \"yah\", \"iyaa\",\n",
    "    \"iya\", \"engga\", \"nggak\", \"ngga\", \"gak\", 'amp', 'all', 'h', 'gt', 'yak', 'rida','tri'\n",
    "}\n",
    "\n",
    "stop_words.update(stopwords_extra)\n",
    "\n",
    "import json\n",
    "with open(\"dopping.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    abbreviation_dict = json.load(f)\n",
    "\n",
    "bentuk_kata = re.compile(r'^[a-z]+$')\n",
    "\n",
    "def reduce_repeated_chars(word):\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', word)\n",
    "\n",
    "def remove_non_latin(text):\n",
    "    return ''.join(\n",
    "        c for c in text \n",
    "        if unicodedata.category(c)[0] != 'C'\n",
    "        and \"LATIN\" in unicodedata.name(c, 'LATIN')\n",
    "    )\n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "def remove_non_latin(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', ' ', text)  \n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+', ' ', text)\n",
    "    text = emoji.replace_emoji(text, replace=' ')\n",
    "    text = re.sub(r'[@#]\\w+', ' ', text)\n",
    "\n",
    "    text = remove_non_latin(text)\n",
    "\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "    words = text.split()\n",
    "    cleaned = []\n",
    "\n",
    "    for w in words:\n",
    "        w = reduce_repeated_chars(w)\n",
    "\n",
    "        if w in abbreviation_dict:\n",
    "            w = abbreviation_dict[w]\n",
    "\n",
    "        if not bentuk_kata.match(w):\n",
    "            continue\n",
    "\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "\n",
    "        w = stemmer.stem(w)\n",
    "\n",
    "        if w in stop_words:\n",
    "            continue\n",
    "\n",
    "        cleaned.append(w)\n",
    "\n",
    "    cleaned_text = \" \".join(cleaned)\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d703902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      smga kuat alam jujur dahsyat kuat dahsyat muda...\n",
       "1      wamen angkat pres susah ngaturnya klau dirjen ...\n",
       "2      uang pakai putar gerak ekonomi masyarakat simp...\n",
       "3      beda sang uang ekonomi nasional lambat lesu in...\n",
       "4                  new idol has been unlock yudhi sadewa\n",
       "                             ...                        \n",
       "245                    waham kabeh dokter pasien kabuurr\n",
       "246    keren moga omon kerja hasil tunggu aksi pajak ...\n",
       "247    heboh anak sadewa pamer gepok dolar as hasil t...\n",
       "248                                            ngebuzzer\n",
       "249    tolak halus program langgar kode etik bisnis s...\n",
       "Name: bersih, Length: 250, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"bersih\"] = df[\"preprocessing CPMK3\"].apply(preprocess)\n",
    "df['bersih']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df37ac5e",
   "metadata": {},
   "source": [
    "<h1>1. SVM + TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4cdba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Running: C=0.1\n",
      "======================\n",
      "Akurasi: 0.6000\n",
      "\n",
      "======================\n",
      "Running: C=1\n",
      "======================\n",
      "Akurasi: 0.5600\n",
      "\n",
      "======================\n",
      "Running: C=10\n",
      "======================\n",
      "Akurasi: 0.5200\n",
      "\n",
      "======================\n",
      "Running: C=100\n",
      "======================\n",
      "Akurasi: 0.5400\n",
      "\n",
      "======================\n",
      "Running: C=1 + balanced\n",
      "======================\n",
      "Akurasi: 0.5400\n",
      "\n",
      "======================================\n",
      "HASIL TERBAIK\n",
      "======================================\n",
      "Skenario terbaik : C=0.1\n",
      "Akurasi terbaik  : 0.6000\n",
      "\n",
      "Classification Report (BEST MODEL):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.57      0.56        21\n",
      "     neutral       0.60      0.43      0.50         7\n",
      "    positive       0.65      0.68      0.67        22\n",
      "\n",
      "    accuracy                           0.60        50\n",
      "   macro avg       0.60      0.56      0.57        50\n",
      "weighted avg       0.60      0.60      0.60        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "X = df[\"bersih\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "\n",
    "scenarios = [\n",
    "    {\"name\": \"C=0.1\", \"params\": {\"C\": 0.1}},\n",
    "    {\"name\": \"C=1\",   \"params\": {\"C\": 1}},\n",
    "    {\"name\": \"C=10\",  \"params\": {\"C\": 10}},\n",
    "    {\"name\": \"C=100\", \"params\": {\"C\": 100}},\n",
    "    {\"name\": \"C=1 + balanced\", \"params\": {\"C\": 1, \"class_weight\": \"balanced\"}},\n",
    "]\n",
    "\n",
    "results = []\n",
    "best_model = None\n",
    "best_acc = 0\n",
    "best_name = \"\"\n",
    "\n",
    "\n",
    "for sc in scenarios:\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"Running: {sc['name']}\")\n",
    "    print(\"======================\")\n",
    "    \n",
    "    model = LinearSVC(**sc[\"params\"])\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    pred = model.predict(X_test_tfidf)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(f\"Akurasi: {acc:.4f}\")\n",
    "    \n",
    "    results.append((sc[\"name\"], acc))\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = model\n",
    "        best_name = sc[\"name\"]\n",
    "        best_pred = pred\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\"HASIL TERBAIK\")\n",
    "print(\"======================================\")\n",
    "print(f\"Skenario terbaik : {best_name}\")\n",
    "print(f\"Akurasi terbaik  : {best_acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report (BEST MODEL):\")\n",
    "print(classification_report(y_test, best_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08e949c",
   "metadata": {},
   "source": [
    "<h1>2. SVM + Word2Vec</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e254a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Running: Linear C=1\n",
      "==============================\n",
      "Akurasi: 0.4200\n",
      "\n",
      "==============================\n",
      "Running: Linear C=10\n",
      "==============================\n",
      "Akurasi: 0.4200\n",
      "\n",
      "==============================\n",
      "Running: RBF C=1\n",
      "==============================\n",
      "Akurasi: 0.5400\n",
      "\n",
      "==============================\n",
      "Running: RBF C=10\n",
      "==============================\n",
      "Akurasi: 0.4800\n",
      "\n",
      "==============================\n",
      "Running: Poly C=1 d=3\n",
      "==============================\n",
      "Akurasi: 0.3800\n",
      "\n",
      "======================================\n",
      " HASIL TERBAIK SVM + WORD2VEC\n",
      "======================================\n",
      "Skenario terbaik : RBF C=1\n",
      "Akurasi terbaik  : 0.5400\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.38      0.44        21\n",
      "     neutral       0.60      0.43      0.50         7\n",
      "    positive       0.53      0.73      0.62        22\n",
      "\n",
      "    accuracy                           0.54        50\n",
      "   macro avg       0.56      0.51      0.52        50\n",
      "weighted avg       0.54      0.54      0.53        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "sentences = df['bersih'].apply(lambda x: x.split())\n",
    "w2v = Word2Vec(sentences, vector_size=200, min_count=2, window=5)\n",
    "\n",
    "def get_w2v_vector(words):\n",
    "    vecs = [w2v.wv[w] for w in words if w in w2v.wv]\n",
    "    return np.mean(vecs, axis=0) if len(vecs) > 0 else np.zeros(200)\n",
    "\n",
    "X_vec = np.array([get_w2v_vector(x.split()) for x in df['bersih']])\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_vec, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scenarios = [\n",
    "    {\"name\": \"Linear C=1\", \"params\": {\"kernel\": \"linear\", \"C\": 1}},\n",
    "    {\"name\": \"Linear C=10\", \"params\": {\"kernel\": \"linear\", \"C\": 10}},\n",
    "    {\"name\": \"RBF C=1\", \"params\": {\"kernel\": \"rbf\", \"C\": 1, \"gamma\": \"scale\"}},\n",
    "    {\"name\": \"RBF C=10\", \"params\": {\"kernel\": \"rbf\", \"C\": 10, \"gamma\": \"scale\"}},\n",
    "    {\"name\": \"Poly C=1 d=3\", \"params\": {\"kernel\": \"poly\", \"C\": 1, \"degree\": 3}},\n",
    "]\n",
    "\n",
    "best_acc = 0\n",
    "best_pred = None\n",
    "best_name = \"\"\n",
    "best_model = None\n",
    "\n",
    "for sc in scenarios:\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Running: {sc['name']}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    model = SVC(**sc[\"params\"])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "\n",
    "    print(f\"Akurasi: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_pred = pred\n",
    "        best_model = model\n",
    "        best_name = sc[\"name\"]\n",
    "\n",
    "print(\"\\n======================================\")\n",
    "print(\" HASIL TERBAIK SVM + WORD2VEC\")\n",
    "print(\"======================================\")\n",
    "print(f\"Skenario terbaik : {best_name}\")\n",
    "print(f\"Akurasi terbaik  : {best_acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, best_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4f493",
   "metadata": {},
   "source": [
    "<h1>3. Logistic Regression + TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================\n",
      "Running: C=0.1 L2\n",
      "=======================================\n",
      "Akurasi: 0.5000\n",
      "\n",
      "=======================================\n",
      "Running: C=0.5 L2\n",
      "=======================================\n",
      "Akurasi: 0.5800\n",
      "\n",
      "=======================================\n",
      "Running: C=1 L2\n",
      "=======================================\n",
      "Akurasi: 0.6000\n",
      "\n",
      "=======================================\n",
      "Running: C=2 L2\n",
      "=======================================\n",
      "Akurasi: 0.6000\n",
      "\n",
      "=======================================\n",
      "Running: C=5 L2\n",
      "=======================================\n",
      "Akurasi: 0.5600\n",
      "\n",
      "=======================================\n",
      "Running: C=1 L1\n",
      "=======================================\n",
      "Akurasi: 0.4400\n",
      "\n",
      "=======================================\n",
      "Running: C=2 L1\n",
      "=======================================\n",
      "Akurasi: 0.4000\n",
      "\n",
      "=======================================\n",
      "Running: SAGA ElasticNet 0.3\n",
      "=======================================\n",
      "Akurasi: 0.4800\n",
      "\n",
      "=======================================\n",
      "Running: SAGA ElasticNet 0.5\n",
      "=======================================\n",
      "Akurasi: 0.4600\n",
      "\n",
      "=======================================\n",
      "Running: SAGA ElasticNet 0.7\n",
      "=======================================\n",
      "Akurasi: 0.4000\n",
      "\n",
      "=======================================\n",
      "HASIL TERBAIK Logistic Regression + TFIDF\n",
      "=======================================\n",
      "Skenario terbaik : C=1 L2\n",
      "Akurasi terbaik  : 0.6000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.57      0.56        21\n",
      "     neutral       0.60      0.43      0.50         7\n",
      "    positive       0.65      0.68      0.67        22\n",
      "\n",
      "    accuracy                           0.60        50\n",
      "   macro avg       0.60      0.56      0.57        50\n",
      "weighted avg       0.60      0.60      0.60        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "X = df[\"bersih\"].astype(str)\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=8000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "scenarios = [\n",
    "    {\"name\": \"C=0.1 L2\", \"params\": {\"C\": 0.1, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"max_iter\": 500}},\n",
    "    {\"name\": \"C=0.5 L2\", \"params\": {\"C\": 0.5, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"max_iter\": 500}},\n",
    "    {\"name\": \"C=1 L2\", \"params\": {\"C\": 1, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"max_iter\": 500}},\n",
    "    {\"name\": \"C=2 L2\", \"params\": {\"C\": 2, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"max_iter\": 500}},\n",
    "    {\"name\": \"C=5 L2\", \"params\": {\"C\": 5, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"max_iter\": 500}},\n",
    "    \n",
    "    {\"name\": \"C=1 L1\", \"params\": {\"C\": 1, \"penalty\": \"l1\", \"solver\": \"liblinear\", \"max_iter\": 500}},\n",
    "    {\"name\": \"C=2 L1\", \"params\": {\"C\": 2, \"penalty\": \"l1\", \"solver\": \"liblinear\", \"max_iter\": 500}},\n",
    "    \n",
    "    {\"name\": \"SAGA ElasticNet 0.3\", \"params\": {\"C\": 1, \"penalty\": \"elasticnet\", \"l1_ratio\": 0.3, \"solver\": \"saga\", \"max_iter\": 500}},\n",
    "    {\"name\": \"SAGA ElasticNet 0.5\", \"params\": {\"C\": 1, \"penalty\": \"elasticnet\", \"l1_ratio\": 0.5, \"solver\": \"saga\", \"max_iter\": 500}},\n",
    "    {\"name\": \"SAGA ElasticNet 0.7\", \"params\": {\"C\": 1, \"penalty\": \"elasticnet\", \"l1_ratio\": 0.7, \"solver\": \"saga\", \"max_iter\": 500}},\n",
    "]\n",
    "\n",
    "best_acc = 0\n",
    "best_model = None\n",
    "best_name = \"\"\n",
    "best_pred = None\n",
    "\n",
    "\n",
    "for sc in scenarios:\n",
    "    print(\"\\n=======================================\")\n",
    "    print(f\"Running: {sc['name']}\")\n",
    "    print(\"=======================================\")\n",
    "\n",
    "    model = LogisticRegression(**sc[\"params\"])\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    pred = model.predict(X_test_tfidf)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "\n",
    "    print(f\"Akurasi: {acc:.4f}\")\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = model\n",
    "        best_pred = pred\n",
    "        best_name = sc[\"name\"]\n",
    "\n",
    "print(\"\\n=======================================\")\n",
    "print(\"HASIL TERBAIK Logistic Regression + TFIDF\")\n",
    "print(\"=======================================\")\n",
    "print(f\"Skenario terbaik : {best_name}\")\n",
    "print(f\"Akurasi terbaik  : {best_acc:.4f}\\n\")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, best_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbd21a8",
   "metadata": {},
   "source": [
    "<h1>4. Logistic Regression + Word2Vec</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca0bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best Parameters: {'C': 20, 'max_iter': 300, 'penalty': 'l2', 'solver': 'saga'}\n",
      "\n",
      "Best Score: 0.2694986007679506\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.95      0.58        21\n",
      "     neutral       0.00      0.00      0.00         7\n",
      "    positive       0.00      0.00      0.00        22\n",
      "\n",
      "    accuracy                           0.40        50\n",
      "   macro avg       0.14      0.32      0.19        50\n",
      "weighted avg       0.17      0.40      0.24        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sentences = df['bersih'].apply(lambda x: x.split())\n",
    "w2v = Word2Vec(sentences, vector_size=200, min_count=2, window=5)\n",
    "\n",
    "def get_w2v_vector(words):\n",
    "    vecs = [w2v.wv[w] for w in words if w in w2v.wv]\n",
    "    return np.mean(vecs, axis=0) if len(vecs) > 0 else np.zeros(200)\n",
    "\n",
    "X_vec = np.array([get_w2v_vector(x.split()) for x in df['bersih']])\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 5, 10, 20],         \n",
    "    \"penalty\": [\"l1\", \"l2\"],                \n",
    "    \"solver\": [\"liblinear\", \"saga\"],       \n",
    "    \"max_iter\": [300, 500]\n",
    "}\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    logreg,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"\\nBest Score:\", grid.best_score_)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78073fc",
   "metadata": {},
   "source": [
    "<h1>5. RNN + Word2Vec</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a52788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRIAL 1/8 =====\n",
      "PARAMETER: {'lstm_units': 64, 'dropout': 0.3, 'lr': 0.0001, 'bidir': True, 'batch': 32, 'maxlen': 80}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 445ms/step\n",
      "Akurasi: 0.44\n",
      "\n",
      "===== TRIAL 2/8 =====\n",
      "PARAMETER: {'lstm_units': 64, 'dropout': 0.3, 'lr': 0.0001, 'bidir': True, 'batch': 32, 'maxlen': 80}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441ms/step\n",
      "Akurasi: 0.44\n",
      "\n",
      "===== TRIAL 3/8 =====\n",
      "PARAMETER: {'lstm_units': 64, 'dropout': 0.4, 'lr': 0.001, 'bidir': False, 'batch': 32, 'maxlen': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "Akurasi: 0.42\n",
      "\n",
      "===== TRIAL 4/8 =====\n",
      "PARAMETER: {'lstm_units': 256, 'dropout': 0.3, 'lr': 0.0005, 'bidir': False, 'batch': 32, 'maxlen': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step\n",
      "Akurasi: 0.44\n",
      "\n",
      "===== TRIAL 5/8 =====\n",
      "PARAMETER: {'lstm_units': 64, 'dropout': 0.4, 'lr': 0.001, 'bidir': False, 'batch': 32, 'maxlen': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step\n",
      "Akurasi: 0.44\n",
      "\n",
      "===== TRIAL 6/8 =====\n",
      "PARAMETER: {'lstm_units': 128, 'dropout': 0.3, 'lr': 0.001, 'bidir': True, 'batch': 16, 'maxlen': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 362ms/step\n",
      "Akurasi: 0.44\n",
      "\n",
      "===== TRIAL 7/8 =====\n",
      "PARAMETER: {'lstm_units': 256, 'dropout': 0.4, 'lr': 0.001, 'bidir': True, 'batch': 16, 'maxlen': 80}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step\n",
      "Akurasi: 0.44\n",
      "\n",
      "===== TRIAL 8/8 =====\n",
      "PARAMETER: {'lstm_units': 64, 'dropout': 0.2, 'lr': 0.0001, 'bidir': False, 'batch': 32, 'maxlen': 60}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 497ms/step\n",
      "Akurasi: 0.44\n",
      "\n",
      "==============================\n",
      "TUNING SELESAI!\n",
      "BEST ACCURACY: 0.44\n",
      "BEST PARAMETER: {'lstm_units': 64, 'dropout': 0.3, 'lr': 0.0001, 'bidir': True, 'batch': 32, 'maxlen': 80}\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "sentences = df['bersih'].apply(lambda x: x.split())\n",
    "w2v = Word2Vec(sentences, vector_size=200, min_count=2, window=5)\n",
    "\n",
    "word_index = {word: i+1 for i, word in enumerate(w2v.wv.index_to_key)}\n",
    "\n",
    "def to_seq(text):\n",
    "    return [word_index[w] for w in text.split() if w in word_index]\n",
    "\n",
    "X_seq = [to_seq(x) for x in df['bersih']]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(df[\"label\"])\n",
    "y_cat = to_categorical(y_enc)\n",
    "\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X_seq, y_cat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index)+1, 200))\n",
    "for w, i in word_index.items():\n",
    "    embedding_matrix[i] = w2v.wv[w]\n",
    "\n",
    "param_space = {\n",
    "    \"lstm_units\": [64, 128, 256],\n",
    "    \"dropout\": [0.2, 0.3, 0.4],\n",
    "    \"lr\": [1e-3, 5e-4, 1e-4],\n",
    "    \"bidir\": [True, False],\n",
    "    \"batch\": [16, 32],\n",
    "    \"maxlen\": [50, 60, 80]\n",
    "}\n",
    "\n",
    "def build_model(lstm_units, dropout, lr, bidir, maxlen):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            input_dim=len(word_index)+1,\n",
    "            output_dim=200,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=maxlen,\n",
    "            trainable=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if bidir:\n",
    "        model.add(Bidirectional(LSTM(lstm_units)))\n",
    "    else:\n",
    "        model.add(LSTM(lstm_units))\n",
    "\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Dense(len(le.classes_), activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=AdamW(learning_rate=lr),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "\n",
    "N_TRIALS = 8  \n",
    "for trial in range(N_TRIALS):\n",
    "    print(f\"\\n===== TRIAL {trial+1}/{N_TRIALS} =====\")\n",
    "\n",
    "    params = {k: random.choice(v) for k, v in param_space.items()}\n",
    "    print(\"PARAMETER:\", params)\n",
    "\n",
    "    X_train = pad_sequences(X_train_raw, maxlen=params[\"maxlen\"])\n",
    "    X_test = pad_sequences(X_test_raw, maxlen=params[\"maxlen\"])\n",
    "\n",
    "    model = build_model(\n",
    "        lstm_units=params[\"lstm_units\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        lr=params[\"lr\"],\n",
    "        bidir=params[\"bidir\"],\n",
    "        maxlen=params[\"maxlen\"]\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=15,\n",
    "        batch_size=params[\"batch\"], \n",
    "        validation_split=0.1,\n",
    "        callbacks=[es],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    true = np.argmax(y_test, axis=1)\n",
    "    acc = accuracy_score(true, pred)\n",
    "\n",
    "    print(\"Akurasi:\", acc)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = params\n",
    "\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"TUNING SELESAI!\")\n",
    "print(\"BEST ACCURACY:\", best_acc)\n",
    "print(\"BEST PARAMETER:\", best_params)\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdceb667",
   "metadata": {},
   "source": [
    "<h1>6. IndoBERT + TF-IDF</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aa4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menghitung embedding BERT...\n",
      "\n",
      "=== MULAI RANDOM SEARCH ===\n",
      "\n",
      "TRIAL 1/10\n",
      "PARAMETER: {'tfidf_max': 7000, 'ngram': (1, 2), 'C': 10, 'loss': 'hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 0.5\n",
      "\n",
      "TRIAL 2/10\n",
      "PARAMETER: {'tfidf_max': 10000, 'ngram': (1, 2), 'C': 5, 'loss': 'squared_hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 0.5\n",
      "\n",
      "TRIAL 3/10\n",
      "PARAMETER: {'tfidf_max': 10000, 'ngram': (1, 2), 'C': 10, 'loss': 'squared_hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 0.48\n",
      "\n",
      "TRIAL 4/10\n",
      "PARAMETER: {'tfidf_max': 5000, 'ngram': (1, 1), 'C': 0.1, 'loss': 'hinge'}\n",
      "Akurasi: 0.46\n",
      "\n",
      "TRIAL 5/10\n",
      "PARAMETER: {'tfidf_max': 10000, 'ngram': (1, 2), 'C': 5, 'loss': 'squared_hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 0.48\n",
      "\n",
      "TRIAL 6/10\n",
      "PARAMETER: {'tfidf_max': 5000, 'ngram': (1, 2), 'C': 1, 'loss': 'squared_hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 0.48\n",
      "\n",
      "TRIAL 7/10\n",
      "PARAMETER: {'tfidf_max': 7000, 'ngram': (1, 1), 'C': 0.1, 'loss': 'squared_hinge'}\n",
      "Akurasi: 0.48\n",
      "\n",
      "TRIAL 8/10\n",
      "PARAMETER: {'tfidf_max': 10000, 'ngram': (1, 1), 'C': 1, 'loss': 'hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 0.5\n",
      "\n",
      "TRIAL 9/10\n",
      "PARAMETER: {'tfidf_max': 10000, 'ngram': (1, 2), 'C': 5, 'loss': 'squared_hinge'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi: 0.48\n",
      "\n",
      "TRIAL 10/10\n",
      "PARAMETER: {'tfidf_max': 7000, 'ngram': (1, 2), 'C': 2, 'loss': 'hinge'}\n",
      "Akurasi: 0.5\n",
      "\n",
      "===================================\n",
      "=== HASIL AKHIR TUNING ===\n",
      "Best Accuracy: 0.5\n",
      "Best Parameter: {'tfidf_max': 7000, 'ngram': (1, 2), 'C': 10, 'loss': 'hinge'}\n",
      "\n",
      "=== Classification Report Terbaik ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.38      0.44        21\n",
      "     neutral       0.42      0.71      0.53         7\n",
      "    positive       0.52      0.55      0.53        22\n",
      "\n",
      "    accuracy                           0.50        50\n",
      "   macro avg       0.49      0.55      0.50        50\n",
      "weighted avg       0.51      0.50      0.50        50\n",
      "\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import random\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "bert = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "def get_bert_cls(text):\n",
    "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        output = bert(**tokens)\n",
    "    return output.last_hidden_state[:,0,:].numpy().flatten()\n",
    "\n",
    "print(\"Menghitung embedding BERT...\")\n",
    "bert_vec = np.array([get_bert_cls(t) for t in df['bersih']])\n",
    "\n",
    "y = df[\"label\"].values\n",
    "\n",
    "param_space = {\n",
    "    \"tfidf_max\": [3000, 5000, 7000, 10000],\n",
    "    \"ngram\": [(1,1), (1,2)],\n",
    "    \"C\": [0.1, 1, 2, 5, 10],\n",
    "    \"loss\": [\"hinge\", \"squared_hinge\"]\n",
    "}\n",
    "\n",
    "best_acc = 0\n",
    "best_params = None\n",
    "best_report = None\n",
    "\n",
    "N_TRIALS = 10\n",
    "print(\"\\n=== MULAI RANDOM SEARCH ===\")\n",
    "\n",
    "for trial in range(N_TRIALS):\n",
    "    print(f\"\\nTRIAL {trial+1}/{N_TRIALS}\")\n",
    "\n",
    "    params = {\n",
    "        key: random.choice(val)\n",
    "        for key, val in param_space.items()\n",
    "    }\n",
    "    print(\"PARAMETER:\", params)\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=params[\"tfidf_max\"],\n",
    "        ngram_range=params[\"ngram\"]\n",
    "    )\n",
    "\n",
    "    tfidf_vec = tfidf.fit_transform(df[\"bersih\"]).toarray()\n",
    "\n",
    "    X = np.hstack([tfidf_vec, bert_vec])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    model = LinearSVC(\n",
    "        C=params[\"C\"],\n",
    "        loss=params[\"loss\"],\n",
    "        max_iter=5000\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    print(\"Akurasi:\", acc)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_params = params\n",
    "        best_report = classification_report(y_test, pred)\n",
    "\n",
    "print(\"\\n===================================\")\n",
    "print(\"=== HASIL AKHIR TUNING ===\")\n",
    "print(\"Best Accuracy:\", best_acc)\n",
    "print(\"Best Parameter:\", best_params)\n",
    "print(\"\\n=== Classification Report Terbaik ===\")\n",
    "print(best_report)\n",
    "print(\"===================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
